{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# DBSCAN + Ridge Regression with Windowing for Anomaly Detection\n",
    "\n",
    "This notebook implements an advanced anomaly detection approach combining:\n",
    "- **DBSCAN clustering** to identify unusual operating states\n",
    "- **Ridge regression** to detect unexpected thermal behavior\n",
    "- **Windowing with statistical features** to capture temporal patterns\n",
    "- **Automatic window size selection** based on clustering quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../data/ETTh1.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "power_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate power features\n",
    "P_high = np.sqrt(df['HUFL']**2 + df['HULL']**2)\n",
    "P_mid = np.sqrt(df['MUFL']**2 + df['MULL']**2)\n",
    "P_low = np.sqrt(df['LUFL']**2 + df['LULL']**2)\n",
    "\n",
    "print(\"Power features calculated:\")\n",
    "print(f\"  P_high: mean={P_high.mean():.2f}, std={P_high.std():.2f}\")\n",
    "print(f\"  P_mid:  mean={P_mid.mean():.2f}, std={P_mid.std():.2f}\")\n",
    "print(f\"  P_low:  mean={P_low.mean():.2f}, std={P_low.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "windowing_section",
   "metadata": {},
   "source": [
    "## 2. Windowing & Statistical Feature Engineering\n",
    "\n",
    "Create rolling window features to capture temporal patterns and dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "windowing_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_features(df, window_sizes=[6, 12, 24]):\n",
    "    \"\"\"\n",
    "    Create statistical features over different window sizes\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with power and temperature features\n",
    "        window_sizes: List of window sizes to try (in hours)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of DataFrames with windowed features for each window size\n",
    "    \"\"\"\n",
    "    base_features = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        print(f\"\\nProcessing window size: {window} hours\")\n",
    "        df_temp = df.copy()\n",
    "        \n",
    "        # Calculate rolling statistics for each feature\n",
    "        for feat in base_features:\n",
    "            df_temp[f'{feat}_mean_{window}h'] = df_temp[feat].rolling(window, min_periods=1).mean()\n",
    "            df_temp[f'{feat}_std_{window}h'] = df_temp[feat].rolling(window, min_periods=1).std().fillna(0)\n",
    "            df_temp[f'{feat}_min_{window}h'] = df_temp[feat].rolling(window, min_periods=1).min()\n",
    "            df_temp[f'{feat}_max_{window}h'] = df_temp[feat].rolling(window, min_periods=1).max()\n",
    "            df_temp[f'{feat}_range_{window}h'] = df_temp[f'{feat}_max_{window}h'] - df_temp[f'{feat}_min_{window}h']\n",
    "        \n",
    "        # Additional power-based features\n",
    "        df_temp[f'P_high_mean_{window}h'] = P_high.rolling(window, min_periods=1).mean()\n",
    "        df_temp[f'P_mid_mean_{window}h'] = P_mid.rolling(window, min_periods=1).mean()\n",
    "        df_temp[f'P_low_mean_{window}h'] = P_low.rolling(window, min_periods=1).mean()\n",
    "        \n",
    "        # Rate of change features (temporal dynamics)\n",
    "        df_temp[f'OT_roc_{window}h'] = df_temp['OT'].diff(window).fillna(0)\n",
    "        df_temp[f'P_high_roc_{window}h'] = P_high.diff(window).fillna(0)\n",
    "        \n",
    "        results[window] = df_temp\n",
    "        \n",
    "        # Count features created\n",
    "        new_cols = [c for c in df_temp.columns if f'_{window}h' in c]\n",
    "        print(f\"  Created {len(new_cols)} windowed features\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create windowed features for different time scales\n",
    "windowed_dfs = create_windowed_features(df.copy(), window_sizes=[6, 12, 24])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "epsilon_section",
   "metadata": {},
   "source": [
    "## 3. Find Optimal DBSCAN Epsilon\n",
    "\n",
    "Use k-distance plot to automatically determine the optimal epsilon parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "epsilon_calc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use base features to find epsilon\n",
    "features = np.c_[P_high, P_mid, P_low, df['OT']]\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "\n",
    "k = 5\n",
    "nbrs = NearestNeighbors(n_neighbors=k)\n",
    "nbrs.fit(X)\n",
    "distances, _ = nbrs.kneighbors(X)\n",
    "k_distances = np.sort(distances[:, k-1])\n",
    "\n",
    "# Calculate elbow point\n",
    "x = np.arange(len(k_distances))\n",
    "y = k_distances\n",
    "point1 = np.array([x[0], y[0]])\n",
    "point2 = np.array([x[-1], y[-1]])\n",
    "distances_to_line = np.abs(\n",
    "    np.cross(point2 - point1, point1 - np.vstack((x, y)).T)\n",
    ") / np.linalg.norm(point2 - point1)\n",
    "elbow_index = np.argmax(distances_to_line)\n",
    "eps_optimal = k_distances[elbow_index]\n",
    "\n",
    "print(f\"Optimal epsilon: {eps_optimal:.4f}\")\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(k_distances, linewidth=1)\n",
    "plt.axhline(eps_optimal, color='red', linestyle='--', \n",
    "            label=f'Optimal ε = {eps_optimal:.4f}')\n",
    "plt.scatter([elbow_index], [eps_optimal], color='red', s=100, zorder=5)\n",
    "plt.xlabel('Points (sorted)')\n",
    "plt.ylabel(f'Distance to {k}th nearest neighbor')\n",
    "plt.title('k-distance Plot for Epsilon Selection')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_section",
   "metadata": {},
   "source": [
    "## 4. Evaluate Different Window Sizes\n",
    "\n",
    "Test each window size and select the one with the best clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_window_size(windowed_dfs, eps_optimal):\n",
    "    \"\"\"\n",
    "    Evaluate clustering quality for each window size\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    for window, df_temp in windowed_dfs.items():\n",
    "        # Select windowed features (exclude OT features for clustering)\n",
    "        feature_cols = [col for col in df_temp.columns \n",
    "                       if f'_{window}h' in col and 'OT' not in col]\n",
    "        \n",
    "        X_window = df_temp[feature_cols].values\n",
    "        \n",
    "        # Standardize\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_window)\n",
    "        \n",
    "        # Run DBSCAN\n",
    "        clustering = DBSCAN(eps=eps_optimal, min_samples=5)\n",
    "        labels = clustering.fit_predict(X_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        noise_ratio = np.mean(labels == -1)\n",
    "        \n",
    "        # Silhouette score (measure of cluster quality)\n",
    "        if n_clusters > 1:\n",
    "            mask = labels != -1\n",
    "            if mask.sum() > 10:\n",
    "                sil_score = silhouette_score(X_scaled[mask], labels[mask])\n",
    "            else:\n",
    "                sil_score = -1\n",
    "        else:\n",
    "            sil_score = -1\n",
    "        \n",
    "        scores[window] = {\n",
    "            'n_clusters': n_clusters,\n",
    "            'noise_ratio': noise_ratio,\n",
    "            'silhouette': sil_score,\n",
    "            'X_scaled': X_scaled,\n",
    "            'labels': labels\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nWindow {window}h:\")\n",
    "        print(f\"  Clusters: {n_clusters}\")\n",
    "        print(f\"  Noise: {noise_ratio:.2%}\")\n",
    "        print(f\"  Silhouette: {sil_score:.3f}\")\n",
    "    \n",
    "    # Select best window (highest silhouette score)\n",
    "    best_window = max(scores.keys(), key=lambda k: scores[k]['silhouette'])\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ Best window size: {best_window} hours (Silhouette: {scores[best_window]['silhouette']:.3f})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return best_window, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_window, window_scores = evaluate_window_size(windowed_dfs, eps_optimal)\n",
    "df_best = windowed_dfs[best_window].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anomaly_section",
   "metadata": {},
   "source": [
    "## 5. Final Anomaly Detection\n",
    "\n",
    "Combine DBSCAN clustering with Ridge regression residuals for comprehensive anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anomaly_detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best features and clustering results\n",
    "X_best = window_scores[best_window]['X_scaled']\n",
    "labels_best = window_scores[best_window]['labels']\n",
    "df_best['cluster'] = labels_best\n",
    "\n",
    "# Ridge regression on windowed features\n",
    "feature_cols = [col for col in df_best.columns \n",
    "               if f'_{best_window}h' in col and 'OT' not in col]\n",
    "X_ridge = df_best[feature_cols].values\n",
    "\n",
    "reg = Ridge(alpha=1.0)\n",
    "reg.fit(X_ridge, df_best['OT'])\n",
    "df_best['OT_pred'] = reg.predict(X_ridge)\n",
    "df_best['residual'] = df_best['OT'] - df_best['OT_pred']\n",
    "\n",
    "# Residual-based anomaly detection using MAD\n",
    "median = np.median(df_best['residual'])\n",
    "mad = np.median(np.abs(df_best['residual'] - median))\n",
    "res_anomaly = np.abs(df_best['residual'] - median) > 3 * mad\n",
    "\n",
    "print(f\"Residual statistics:\")\n",
    "print(f\"  Median: {median:.4f}\")\n",
    "print(f\"  MAD: {mad:.4f}\")\n",
    "print(f\"  3×MAD threshold: {3*mad:.4f}\")\n",
    "print(f\"  High residual points: {res_anomaly.sum()} ({res_anomaly.mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combine_anomalies",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined anomaly detection (NO temperature threshold!)\n",
    "# An anomaly is either:\n",
    "# 1. DBSCAN noise point with high residual (unusual state + unexpected response)\n",
    "# 2. Any point with very high residual (unexpected thermal behavior)\n",
    "\n",
    "dbscan_anomaly = (df_best['cluster'] == -1) & res_anomaly\n",
    "high_residual_anomaly = np.abs(df_best['residual'] - median) > 4 * mad\n",
    "\n",
    "final_anomaly = dbscan_anomaly | high_residual_anomaly\n",
    "\n",
    "# Categorize anomalies for analysis\n",
    "df_best['anomaly_type'] = 'Normal'\n",
    "df_best.loc[dbscan_anomaly & high_residual_anomaly, 'anomaly_type'] = 'Both'\n",
    "df_best.loc[dbscan_anomaly & ~high_residual_anomaly, 'anomaly_type'] = 'DBSCAN Only'\n",
    "df_best.loc[~dbscan_anomaly & high_residual_anomaly, 'anomaly_type'] = 'Residual Only'\n",
    "\n",
    "print(f\"\\nAnomaly Detection Summary:\")\n",
    "print(f\"  Total anomalies: {final_anomaly.sum()} ({final_anomaly.mean()*100:.2f}%)\")\n",
    "print(f\"\\nBreakdown by type:\")\n",
    "for anom_type in ['Both', 'DBSCAN Only', 'Residual Only']:\n",
    "    count = (df_best['anomaly_type'] == anom_type).sum()\n",
    "    pct = count / len(df_best) * 100\n",
    "    print(f\"  {anom_type:15s}: {count:4d} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_section",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Temperature with anomalies by type\n",
    "axes[0].plot(df_best['date'], df_best['OT'], label='OT', alpha=0.7, linewidth=0.8, color='blue')\n",
    "\n",
    "# Plot different anomaly types with different colors\n",
    "colors = {'Both': 'red', 'DBSCAN Only': 'orange', 'Residual Only': 'purple'}\n",
    "for anom_type, color in colors.items():\n",
    "    mask = df_best['anomaly_type'] == anom_type\n",
    "    if mask.sum() > 0:\n",
    "        axes[0].scatter(\n",
    "            df_best.loc[mask, 'date'], \n",
    "            df_best.loc[mask, 'OT'],\n",
    "            color=color, s=30, label=anom_type, zorder=5, alpha=0.7\n",
    "        )\n",
    "\n",
    "axes[0].set_ylabel('Temperature (OT)')\n",
    "axes[0].set_title(f'Anomaly Detection by Type (Window={best_window}h)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "axes[1].plot(df_best['date'], df_best['residual'], label='Residuals', alpha=0.6, linewidth=0.8)\n",
    "axes[1].axhline(median + 3*mad, color='orange', linestyle='--', \n",
    "                linewidth=1, label='±3×MAD', alpha=0.7)\n",
    "axes[1].axhline(median - 3*mad, color='orange', linestyle='--', linewidth=1, alpha=0.7)\n",
    "axes[1].axhline(median + 4*mad, color='red', linestyle='--', \n",
    "                linewidth=1, label='±4×MAD', alpha=0.7)\n",
    "axes[1].axhline(median - 4*mad, color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
    "axes[1].scatter(\n",
    "    df_best.loc[final_anomaly, 'date'],\n",
    "    df_best.loc[final_anomaly, 'residual'],\n",
    "    color='red', s=30, zorder=5, alpha=0.5\n",
    ")\n",
    "axes[1].set_ylabel('Residual')\n",
    "axes[1].set_title('Prediction Residuals')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Temperature variability (rolling std)\n",
    "std_col = f'OT_std_{best_window}h'\n",
    "axes[2].plot(df_best['date'], df_best[std_col], \n",
    "             label=f'OT Std ({best_window}h)', color='purple', linewidth=0.8)\n",
    "axes[2].scatter(\n",
    "    df_best.loc[final_anomaly, 'date'],\n",
    "    df_best.loc[final_anomaly, std_col],\n",
    "    color='red', s=30, zorder=5, alpha=0.5\n",
    ")\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Rolling Std')\n",
    "axes[2].set_title('Temperature Variability')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_section",
   "metadata": {},
   "source": [
    "## 7. Final Results & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL ANOMALY DETECTION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nBest Window Size: {best_window} hours\")\n",
    "\n",
    "print(f\"\\nDBSCAN Clustering:\")\n",
    "n_clusters = len(set(labels_best)) - (1 if -1 in labels_best else 0)\n",
    "noise_count = np.sum(labels_best == -1)\n",
    "print(f\"  Clusters: {n_clusters}\")\n",
    "print(f\"  Noise points: {noise_count} ({noise_count/len(df_best)*100:.2f}%)\")\n",
    "print(f\"  Silhouette score: {window_scores[best_window]['silhouette']:.3f}\")\n",
    "\n",
    "print(f\"\\nResidual Analysis:\")\n",
    "print(f\"  Median: {median:.4f}\")\n",
    "print(f\"  MAD: {mad:.4f}\")\n",
    "print(f\"  3×MAD threshold: ±{3*mad:.4f}\")\n",
    "print(f\"  High residual points: {res_anomaly.sum()} ({res_anomaly.mean()*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nFinal Anomalies:\")\n",
    "print(f\"  Total: {final_anomaly.sum()} ({final_anomaly.mean()*100:.2f}%)\")\n",
    "print(f\"\\nAnomaly Breakdown:\")\n",
    "for anom_type in ['Both', 'DBSCAN Only', 'Residual Only']:\n",
    "    count = (df_best['anomaly_type'] == anom_type).sum()\n",
    "    pct = count / len(df_best) * 100\n",
    "    print(f\"  {anom_type:15s}: {count:4d} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save anomalies to CSV\n",
    "anomaly_df = df_best[final_anomaly][['date', 'OT', 'residual', 'cluster', 'anomaly_type'] + \n",
    "                                      [f'OT_std_{best_window}h', \n",
    "                                       f'OT_range_{best_window}h']].copy()\n",
    "anomaly_df = anomaly_df.sort_values('date')\n",
    "anomaly_df.to_csv('windowed_anomalies.csv', index=False)\n",
    "print(f\"\\n✓ {final_anomaly.sum()} anomalies saved to 'windowed_anomalies.csv'\")\n",
    "\n",
    "# Display sample anomalies\n",
    "print(\"\\nSample anomalies:\")\n",
    "anomaly_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size comparison summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WINDOW SIZE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_data = []\n",
    "for window in sorted(window_scores.keys()):\n",
    "    scores = window_scores[window]\n",
    "    comparison_data.append({\n",
    "        'Window (h)': window,\n",
    "        'Clusters': scores['n_clusters'],\n",
    "        'Noise %': f\"{scores['noise_ratio']*100:.2f}\",\n",
    "        'Silhouette': f\"{scores['silhouette']:.3f}\",\n",
    "        'Best': '✓' if window == best_window else ''\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This approach successfully detects anomalies by:\n",
    "\n",
    "1. **Temporal Context**: Windowing captures patterns that point-based methods miss\n",
    "2. **Multi-criteria Detection**: Combines clustering (unusual states) with regression (unexpected responses)\n",
    "3. **Automatic Optimization**: Selects best window size based on clustering quality\n",
    "4. **No Arbitrary Thresholds**: Removed the 85th percentile filter to catch all anomalies\n",
    "\n",
    "### Anomaly Types:\n",
    "- **Both**: Most severe - unusual operating state AND high prediction error\n",
    "- **DBSCAN Only**: Rare but predictable states\n",
    "- **Residual Only**: Common states with unexpected thermal behavior\n",
    "\n",
    "The windowed features capture:\n",
    "- Gradual drift before failures\n",
    "- Increased variability during abnormal operations  \n",
    "- Rate of change patterns\n",
    "- Statistical properties over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
